{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeGCFiXh8cC5",
        "outputId": "e63f7d4f-de88-4e84-ae63-e2d515653182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDvpGhTZ9SwA"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4tpqEpz8ia5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Enable faster training on NVIDIA GPUs\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkuH-8zC9VVn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wNax33z9WPb"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, initial_channels=64, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = initial_channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(initial_channels)\n",
        "\n",
        "        # Stacking layers\n",
        "        self.layer1 = self._make_layer(block, initial_channels,     num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, initial_channels*2,   num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, initial_channels*4,   num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, initial_channels*8,   num_blocks[3], stride=2)\n",
        "\n",
        "        self.linear = nn.Linear(initial_channels*8*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_planes, planes, s))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)  # typical for CIFAR-10\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNetTeacher():\n",
        "    \"\"\"\n",
        "    A \"larger\" teacher with more layers, e.g. a ResNet-like config [3,4,6,3].\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], initial_channels=64, num_classes=10)\n",
        "\n",
        "\n",
        "def ResNetStudent():\n",
        "    \"\"\"\n",
        "    A smaller student, e.g. [2,2,2,1] with fewer channels.\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 1], initial_channels=56, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVhU75i-9e4b"
      },
      "outputs": [],
      "source": [
        "def ResNetTeacher():\n",
        "    \"\"\"\n",
        "    Example \"bigger\" teacher:\n",
        "    4 stages with [3, 4, 6, 3] blocks each,\n",
        "    initial_channels=64 => total params are bigger than the student.\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], initial_channels=64, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9O5uQZi9rFQ"
      },
      "outputs": [],
      "source": [
        "def ResNetStudent():\n",
        "    \"\"\"\n",
        "    A smaller student model with fewer blocks / channels:\n",
        "    e.g. [2, 2, 2, 1], initial_channels=56\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 1], initial_channels=56, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOFK9-Ul9t4p"
      },
      "outputs": [],
      "source": [
        "def distillation_loss(student_logits, teacher_logits, labels,\n",
        "                      alpha=0.5, T=4.0, label_smoothing=0.0):\n",
        "    \"\"\"\n",
        "    student_logits: Student's raw output\n",
        "    teacher_logits: Teacher's raw output (no gradient)\n",
        "    labels: Ground-truth labels\n",
        "    alpha: weighting factor between CE and Distillation\n",
        "    T: temperature\n",
        "    label_smoothing: optional label smoothing for CE\n",
        "    \"\"\"\n",
        "    # CE with label smoothing\n",
        "    ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing)(student_logits, labels)\n",
        "\n",
        "    # KL divergence of softened outputs\n",
        "    kl = nn.KLDivLoss(reduction='batchmean')(\n",
        "        F.log_softmax(student_logits / T, dim=1),\n",
        "        F.softmax(teacher_logits / T, dim=1)\n",
        "    ) * (T * T)\n",
        "\n",
        "    return alpha * ce + (1 - alpha) * kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHUwzq9XdR3f",
        "outputId": "495b4244-308d-4485-c7da-da4f50670d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher training function ready.\n"
          ]
        }
      ],
      "source": [
        "def train_teacher(\n",
        "    teacher_model,\n",
        "    trainloader,\n",
        "    valloader,\n",
        "    num_epochs=100,\n",
        "    lr=0.1,\n",
        "    label_smoothing=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Standard classification training for the teacher model on CIFAR-10.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = teacher_model.to(device).to(memory_format=torch.channels_last)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        val_acc = validate(model, valloader, device)\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss/len(trainloader):.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"Teacher training function ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zdfu_I89wP9"
      },
      "outputs": [],
      "source": [
        "def train_distilled(teacher, student, trainloader, valloader,\n",
        "                    num_epochs=250, alpha=0.5, T=4.0,\n",
        "                    lr=0.1, label_smoothing=0.1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move teacher to device, set eval mode\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Move student to device, set train mode\n",
        "    student = student.to(device).to(memory_format=torch.channels_last)\n",
        "    student.train()\n",
        "\n",
        "    optimizer = torch.optim.SGD(student.parameters(), lr=lr,\n",
        "                                momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(inputs)\n",
        "\n",
        "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                student_outputs = student(inputs)\n",
        "                loss = distillation_loss(\n",
        "                    student_outputs,\n",
        "                    teacher_outputs,\n",
        "                    labels,\n",
        "                    alpha=alpha,\n",
        "                    T=T,\n",
        "                    label_smoothing=label_smoothing\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping (optional)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = student_outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "        train_acc = 100. * correct / total\n",
        "        val_acc   = validate(student, valloader, device)\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss/len(trainloader):.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return student\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6SoWYd_91rf"
      },
      "outputs": [],
      "source": [
        "def validate(net, dataloader, device):\n",
        "    net.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    net.train()\n",
        "    return 100. * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vLKdfJO-n8s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VYtlGaD93SV"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(batch_size=512, val_ratio=0.1):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # CIFAR10 train/val\n",
        "    cifar_trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform_train\n",
        "    )\n",
        "    train_size = int((1 - val_ratio) * len(cifar_trainset))\n",
        "    val_size = len(cifar_trainset) - train_size\n",
        "    trainset, valset = random_split(cifar_trainset, [train_size, val_size])\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "    valloader   = DataLoader(valset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # CIFAR10 test\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform_test\n",
        "    )\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, valloader, testloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-cpBdcT-0Eg"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        datadict = pickle.load(fo, encoding='bytes')\n",
        "    return datadict\n",
        "\n",
        "def load_eval(filename):\n",
        "    \"\"\"\n",
        "    Example function to load unlabeled images from a .pkl file\n",
        "    that stores { b'ids': ..., b'data': ... }.\n",
        "    \"\"\"\n",
        "    datadict = unpickle(filename)\n",
        "    ids = datadict[b'ids']   # e.g. [0, 1, 2, ...]\n",
        "    imgs = datadict[b'data'] # raw image data, shape might be (N, 32, 32, 3) or something else\n",
        "    imgs = imgs.astype(\"uint8\")\n",
        "    return ids, imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnzh2lGo-1jv"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, ids, images, transform=None):\n",
        "        self.ids = ids\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.ids[idx]\n",
        "        img = Image.fromarray(self.images[idx])  # shape must be (H,W,3)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img_id, img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wu1yn0d-20S"
      },
      "outputs": [],
      "source": [
        "def generate_submission(model, submission_loader, device, out_csv='submission.csv'):\n",
        "    model.eval()\n",
        "    all_ids, all_preds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_ids, inputs in submission_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_ids.extend(img_ids.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    df = pd.DataFrame({'ID': all_ids, 'Labels': all_preds})\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved predictions to {out_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwI9ZtrwmpxK",
        "outputId": "401571ff-6a85-43b8-ae24-721a64690b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y528kOQq-6H_",
        "outputId": "d17d01b4-efd2-4ef5-d22f-de8ca4cb032d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training Teacher ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-bba8bffd9e2b>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-30-bba8bffd9e2b>:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/100] Loss: 1.7348 | Train Acc: 35.52% | Val Acc: 42.48%\n",
            "[Epoch 2/100] Loss: 1.2938 | Train Acc: 53.52% | Val Acc: 48.38%\n",
            "[Epoch 3/100] Loss: 1.0236 | Train Acc: 63.75% | Val Acc: 58.80%\n",
            "[Epoch 4/100] Loss: 0.8769 | Train Acc: 69.26% | Val Acc: 62.76%\n",
            "[Epoch 5/100] Loss: 0.7636 | Train Acc: 73.33% | Val Acc: 67.54%\n",
            "[Epoch 6/100] Loss: 0.7024 | Train Acc: 75.69% | Val Acc: 72.24%\n",
            "[Epoch 7/100] Loss: 0.6353 | Train Acc: 77.97% | Val Acc: 66.60%\n",
            "[Epoch 8/100] Loss: 0.5918 | Train Acc: 79.43% | Val Acc: 74.46%\n",
            "[Epoch 9/100] Loss: 0.5514 | Train Acc: 81.05% | Val Acc: 74.02%\n",
            "[Epoch 10/100] Loss: 0.5175 | Train Acc: 82.04% | Val Acc: 77.14%\n",
            "[Epoch 11/100] Loss: 0.4845 | Train Acc: 83.14% | Val Acc: 79.18%\n",
            "[Epoch 12/100] Loss: 0.4701 | Train Acc: 83.82% | Val Acc: 78.26%\n",
            "[Epoch 13/100] Loss: 0.4420 | Train Acc: 84.85% | Val Acc: 77.56%\n",
            "[Epoch 14/100] Loss: 0.4164 | Train Acc: 85.72% | Val Acc: 77.46%\n",
            "[Epoch 15/100] Loss: 0.4025 | Train Acc: 86.20% | Val Acc: 74.28%\n",
            "[Epoch 16/100] Loss: 0.3918 | Train Acc: 86.37% | Val Acc: 80.92%\n",
            "[Epoch 17/100] Loss: 0.3670 | Train Acc: 87.18% | Val Acc: 82.96%\n",
            "[Epoch 18/100] Loss: 0.3593 | Train Acc: 87.59% | Val Acc: 81.42%\n",
            "[Epoch 19/100] Loss: 0.3470 | Train Acc: 88.02% | Val Acc: 76.70%\n",
            "[Epoch 20/100] Loss: 0.3286 | Train Acc: 88.59% | Val Acc: 84.66%\n",
            "[Epoch 21/100] Loss: 0.3174 | Train Acc: 89.08% | Val Acc: 84.74%\n",
            "[Epoch 22/100] Loss: 0.3050 | Train Acc: 89.44% | Val Acc: 84.64%\n",
            "[Epoch 23/100] Loss: 0.2988 | Train Acc: 89.59% | Val Acc: 83.62%\n",
            "[Epoch 24/100] Loss: 0.2801 | Train Acc: 90.21% | Val Acc: 77.90%\n",
            "[Epoch 25/100] Loss: 0.2809 | Train Acc: 90.25% | Val Acc: 84.54%\n",
            "[Epoch 26/100] Loss: 0.2655 | Train Acc: 90.82% | Val Acc: 85.30%\n",
            "[Epoch 27/100] Loss: 0.2571 | Train Acc: 90.97% | Val Acc: 86.36%\n",
            "[Epoch 28/100] Loss: 0.2479 | Train Acc: 91.35% | Val Acc: 86.56%\n",
            "[Epoch 29/100] Loss: 0.2379 | Train Acc: 91.77% | Val Acc: 84.66%\n",
            "[Epoch 30/100] Loss: 0.2325 | Train Acc: 91.87% | Val Acc: 86.96%\n",
            "[Epoch 31/100] Loss: 0.2302 | Train Acc: 91.85% | Val Acc: 86.74%\n",
            "[Epoch 32/100] Loss: 0.2182 | Train Acc: 92.36% | Val Acc: 85.74%\n",
            "[Epoch 33/100] Loss: 0.2075 | Train Acc: 92.63% | Val Acc: 85.70%\n",
            "[Epoch 34/100] Loss: 0.2036 | Train Acc: 93.00% | Val Acc: 87.14%\n",
            "[Epoch 35/100] Loss: 0.1937 | Train Acc: 93.20% | Val Acc: 86.62%\n",
            "[Epoch 36/100] Loss: 0.1869 | Train Acc: 93.58% | Val Acc: 88.40%\n",
            "[Epoch 37/100] Loss: 0.1824 | Train Acc: 93.65% | Val Acc: 86.30%\n",
            "[Epoch 38/100] Loss: 0.1715 | Train Acc: 94.08% | Val Acc: 87.26%\n",
            "[Epoch 39/100] Loss: 0.1677 | Train Acc: 94.15% | Val Acc: 89.28%\n",
            "[Epoch 40/100] Loss: 0.1581 | Train Acc: 94.40% | Val Acc: 88.50%\n",
            "[Epoch 41/100] Loss: 0.1537 | Train Acc: 94.59% | Val Acc: 89.20%\n",
            "[Epoch 42/100] Loss: 0.1493 | Train Acc: 94.82% | Val Acc: 88.56%\n",
            "[Epoch 43/100] Loss: 0.1495 | Train Acc: 94.84% | Val Acc: 88.28%\n",
            "[Epoch 44/100] Loss: 0.1395 | Train Acc: 95.02% | Val Acc: 88.10%\n",
            "[Epoch 45/100] Loss: 0.1326 | Train Acc: 95.32% | Val Acc: 88.80%\n",
            "[Epoch 46/100] Loss: 0.1298 | Train Acc: 95.56% | Val Acc: 90.18%\n",
            "[Epoch 47/100] Loss: 0.1245 | Train Acc: 95.67% | Val Acc: 87.60%\n",
            "[Epoch 48/100] Loss: 0.1173 | Train Acc: 95.97% | Val Acc: 89.62%\n",
            "[Epoch 49/100] Loss: 0.1116 | Train Acc: 96.19% | Val Acc: 90.00%\n",
            "[Epoch 50/100] Loss: 0.1089 | Train Acc: 96.15% | Val Acc: 89.42%\n",
            "[Epoch 51/100] Loss: 0.1039 | Train Acc: 96.38% | Val Acc: 89.82%\n",
            "[Epoch 52/100] Loss: 0.0989 | Train Acc: 96.58% | Val Acc: 91.00%\n",
            "[Epoch 53/100] Loss: 0.0973 | Train Acc: 96.64% | Val Acc: 90.48%\n",
            "[Epoch 54/100] Loss: 0.0911 | Train Acc: 96.89% | Val Acc: 89.52%\n",
            "[Epoch 55/100] Loss: 0.0895 | Train Acc: 96.92% | Val Acc: 89.20%\n",
            "[Epoch 56/100] Loss: 0.0873 | Train Acc: 96.92% | Val Acc: 89.32%\n",
            "[Epoch 57/100] Loss: 0.0785 | Train Acc: 97.28% | Val Acc: 89.76%\n",
            "[Epoch 58/100] Loss: 0.0764 | Train Acc: 97.32% | Val Acc: 90.08%\n",
            "[Epoch 59/100] Loss: 0.0718 | Train Acc: 97.59% | Val Acc: 90.30%\n",
            "[Epoch 60/100] Loss: 0.0658 | Train Acc: 97.75% | Val Acc: 90.86%\n",
            "[Epoch 61/100] Loss: 0.0637 | Train Acc: 97.87% | Val Acc: 89.08%\n",
            "[Epoch 62/100] Loss: 0.0607 | Train Acc: 97.92% | Val Acc: 89.98%\n",
            "[Epoch 63/100] Loss: 0.0585 | Train Acc: 97.99% | Val Acc: 91.08%\n",
            "[Epoch 64/100] Loss: 0.0539 | Train Acc: 98.15% | Val Acc: 91.74%\n",
            "[Epoch 65/100] Loss: 0.0467 | Train Acc: 98.40% | Val Acc: 90.88%\n",
            "[Epoch 66/100] Loss: 0.0481 | Train Acc: 98.40% | Val Acc: 91.46%\n",
            "[Epoch 67/100] Loss: 0.0443 | Train Acc: 98.57% | Val Acc: 91.62%\n",
            "[Epoch 68/100] Loss: 0.0411 | Train Acc: 98.64% | Val Acc: 91.68%\n",
            "[Epoch 69/100] Loss: 0.0394 | Train Acc: 98.71% | Val Acc: 91.52%\n",
            "[Epoch 70/100] Loss: 0.0371 | Train Acc: 98.70% | Val Acc: 92.22%\n",
            "[Epoch 71/100] Loss: 0.0347 | Train Acc: 98.79% | Val Acc: 91.44%\n",
            "[Epoch 72/100] Loss: 0.0299 | Train Acc: 99.00% | Val Acc: 91.76%\n",
            "[Epoch 73/100] Loss: 0.0278 | Train Acc: 99.08% | Val Acc: 92.16%\n",
            "[Epoch 74/100] Loss: 0.0254 | Train Acc: 99.18% | Val Acc: 91.72%\n",
            "[Epoch 75/100] Loss: 0.0251 | Train Acc: 99.15% | Val Acc: 91.34%\n",
            "[Epoch 76/100] Loss: 0.0220 | Train Acc: 99.29% | Val Acc: 92.38%\n",
            "[Epoch 77/100] Loss: 0.0196 | Train Acc: 99.37% | Val Acc: 92.36%\n",
            "[Epoch 78/100] Loss: 0.0208 | Train Acc: 99.29% | Val Acc: 92.44%\n",
            "[Epoch 79/100] Loss: 0.0160 | Train Acc: 99.51% | Val Acc: 92.66%\n",
            "[Epoch 80/100] Loss: 0.0162 | Train Acc: 99.46% | Val Acc: 92.96%\n",
            "[Epoch 81/100] Loss: 0.0135 | Train Acc: 99.62% | Val Acc: 92.70%\n",
            "[Epoch 82/100] Loss: 0.0114 | Train Acc: 99.67% | Val Acc: 93.06%\n",
            "[Epoch 83/100] Loss: 0.0109 | Train Acc: 99.65% | Val Acc: 93.46%\n",
            "[Epoch 84/100] Loss: 0.0108 | Train Acc: 99.67% | Val Acc: 93.26%\n",
            "[Epoch 85/100] Loss: 0.0095 | Train Acc: 99.71% | Val Acc: 93.04%\n",
            "[Epoch 86/100] Loss: 0.0081 | Train Acc: 99.75% | Val Acc: 93.32%\n",
            "[Epoch 87/100] Loss: 0.0074 | Train Acc: 99.78% | Val Acc: 93.40%\n",
            "[Epoch 88/100] Loss: 0.0077 | Train Acc: 99.79% | Val Acc: 93.26%\n",
            "[Epoch 89/100] Loss: 0.0062 | Train Acc: 99.83% | Val Acc: 93.64%\n",
            "[Epoch 90/100] Loss: 0.0064 | Train Acc: 99.82% | Val Acc: 93.76%\n",
            "[Epoch 91/100] Loss: 0.0053 | Train Acc: 99.86% | Val Acc: 94.00%\n",
            "[Epoch 92/100] Loss: 0.0052 | Train Acc: 99.87% | Val Acc: 92.96%\n",
            "[Epoch 93/100] Loss: 0.0047 | Train Acc: 99.87% | Val Acc: 93.56%\n",
            "[Epoch 94/100] Loss: 0.0043 | Train Acc: 99.89% | Val Acc: 93.22%\n",
            "[Epoch 95/100] Loss: 0.0044 | Train Acc: 99.91% | Val Acc: 93.40%\n",
            "[Epoch 96/100] Loss: 0.0046 | Train Acc: 99.88% | Val Acc: 93.48%\n",
            "[Epoch 97/100] Loss: 0.0049 | Train Acc: 99.87% | Val Acc: 93.44%\n",
            "[Epoch 98/100] Loss: 0.0041 | Train Acc: 99.90% | Val Acc: 93.74%\n",
            "[Epoch 99/100] Loss: 0.0039 | Train Acc: 99.91% | Val Acc: 93.76%\n",
            "[Epoch 100/100] Loss: 0.0042 | Train Acc: 99.91% | Val Acc: 93.76%\n",
            "\n",
            "=== Distillation Training ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-def3589d1581>:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-31-def3589d1581>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/250] Loss: 1.8493 | Train Acc: 32.87% | Val Acc: 40.56%\n",
            "[Epoch 2/250] Loss: 1.4932 | Train Acc: 48.28% | Val Acc: 47.84%\n",
            "[Epoch 3/250] Loss: 1.3465 | Train Acc: 54.93% | Val Acc: 53.50%\n",
            "[Epoch 4/250] Loss: 1.2518 | Train Acc: 59.02% | Val Acc: 60.66%\n",
            "[Epoch 5/250] Loss: 1.1814 | Train Acc: 62.28% | Val Acc: 60.98%\n",
            "[Epoch 6/250] Loss: 1.1257 | Train Acc: 64.31% | Val Acc: 61.86%\n",
            "[Epoch 7/250] Loss: 1.0793 | Train Acc: 66.37% | Val Acc: 63.08%\n",
            "[Epoch 8/250] Loss: 1.0329 | Train Acc: 68.33% | Val Acc: 69.02%\n",
            "[Epoch 9/250] Loss: 0.9923 | Train Acc: 70.10% | Val Acc: 69.90%\n",
            "[Epoch 10/250] Loss: 0.9592 | Train Acc: 71.53% | Val Acc: 68.20%\n",
            "[Epoch 11/250] Loss: 0.9250 | Train Acc: 73.06% | Val Acc: 71.68%\n",
            "[Epoch 12/250] Loss: 0.8975 | Train Acc: 74.33% | Val Acc: 73.06%\n",
            "[Epoch 13/250] Loss: 0.8690 | Train Acc: 75.34% | Val Acc: 73.48%\n",
            "[Epoch 14/250] Loss: 0.8412 | Train Acc: 76.68% | Val Acc: 71.98%\n",
            "[Epoch 15/250] Loss: 0.8215 | Train Acc: 77.24% | Val Acc: 76.32%\n",
            "[Epoch 16/250] Loss: 0.8006 | Train Acc: 78.37% | Val Acc: 76.48%\n",
            "[Epoch 17/250] Loss: 0.7853 | Train Acc: 78.90% | Val Acc: 74.84%\n",
            "[Epoch 18/250] Loss: 0.7659 | Train Acc: 79.82% | Val Acc: 74.24%\n",
            "[Epoch 19/250] Loss: 0.7494 | Train Acc: 80.32% | Val Acc: 79.20%\n",
            "[Epoch 20/250] Loss: 0.7370 | Train Acc: 80.76% | Val Acc: 79.72%\n",
            "[Epoch 21/250] Loss: 0.7225 | Train Acc: 81.40% | Val Acc: 79.60%\n",
            "[Epoch 22/250] Loss: 0.7154 | Train Acc: 81.73% | Val Acc: 76.98%\n",
            "[Epoch 23/250] Loss: 0.6994 | Train Acc: 82.35% | Val Acc: 77.86%\n",
            "[Epoch 24/250] Loss: 0.6869 | Train Acc: 83.05% | Val Acc: 80.00%\n",
            "[Epoch 25/250] Loss: 0.6739 | Train Acc: 83.41% | Val Acc: 78.10%\n",
            "[Epoch 26/250] Loss: 0.6650 | Train Acc: 83.74% | Val Acc: 78.00%\n",
            "[Epoch 27/250] Loss: 0.6540 | Train Acc: 84.27% | Val Acc: 80.40%\n",
            "[Epoch 28/250] Loss: 0.6427 | Train Acc: 84.69% | Val Acc: 80.64%\n",
            "[Epoch 29/250] Loss: 0.6368 | Train Acc: 84.99% | Val Acc: 81.84%\n",
            "[Epoch 30/250] Loss: 0.6274 | Train Acc: 85.20% | Val Acc: 81.42%\n",
            "[Epoch 31/250] Loss: 0.6212 | Train Acc: 85.54% | Val Acc: 81.70%\n",
            "[Epoch 32/250] Loss: 0.6110 | Train Acc: 85.99% | Val Acc: 80.46%\n",
            "[Epoch 33/250] Loss: 0.6001 | Train Acc: 86.20% | Val Acc: 81.54%\n",
            "[Epoch 34/250] Loss: 0.5950 | Train Acc: 86.56% | Val Acc: 81.48%\n",
            "[Epoch 35/250] Loss: 0.5879 | Train Acc: 86.86% | Val Acc: 81.98%\n",
            "[Epoch 36/250] Loss: 0.5775 | Train Acc: 87.24% | Val Acc: 82.88%\n",
            "[Epoch 37/250] Loss: 0.5726 | Train Acc: 87.57% | Val Acc: 83.24%\n",
            "[Epoch 38/250] Loss: 0.5662 | Train Acc: 87.68% | Val Acc: 83.30%\n",
            "[Epoch 39/250] Loss: 0.5603 | Train Acc: 88.03% | Val Acc: 81.90%\n",
            "[Epoch 40/250] Loss: 0.5527 | Train Acc: 88.16% | Val Acc: 82.50%\n",
            "[Epoch 41/250] Loss: 0.5476 | Train Acc: 88.40% | Val Acc: 80.44%\n",
            "[Epoch 42/250] Loss: 0.5373 | Train Acc: 88.84% | Val Acc: 83.92%\n",
            "[Epoch 43/250] Loss: 0.5305 | Train Acc: 89.06% | Val Acc: 82.74%\n",
            "[Epoch 44/250] Loss: 0.5269 | Train Acc: 89.31% | Val Acc: 83.24%\n",
            "[Epoch 45/250] Loss: 0.5220 | Train Acc: 89.50% | Val Acc: 82.78%\n",
            "[Epoch 46/250] Loss: 0.5153 | Train Acc: 89.78% | Val Acc: 84.80%\n",
            "[Epoch 47/250] Loss: 0.5119 | Train Acc: 89.87% | Val Acc: 83.26%\n",
            "[Epoch 48/250] Loss: 0.5048 | Train Acc: 90.15% | Val Acc: 83.28%\n",
            "[Epoch 49/250] Loss: 0.5019 | Train Acc: 90.26% | Val Acc: 84.26%\n",
            "[Epoch 50/250] Loss: 0.4983 | Train Acc: 90.44% | Val Acc: 82.84%\n",
            "[Epoch 51/250] Loss: 0.4879 | Train Acc: 90.79% | Val Acc: 85.34%\n",
            "[Epoch 52/250] Loss: 0.4861 | Train Acc: 90.84% | Val Acc: 84.10%\n",
            "[Epoch 53/250] Loss: 0.4799 | Train Acc: 91.08% | Val Acc: 84.04%\n",
            "[Epoch 54/250] Loss: 0.4800 | Train Acc: 91.19% | Val Acc: 85.08%\n",
            "[Epoch 55/250] Loss: 0.4724 | Train Acc: 91.58% | Val Acc: 83.92%\n",
            "[Epoch 56/250] Loss: 0.4695 | Train Acc: 91.71% | Val Acc: 84.24%\n",
            "[Epoch 57/250] Loss: 0.4648 | Train Acc: 91.67% | Val Acc: 85.26%\n",
            "[Epoch 58/250] Loss: 0.4592 | Train Acc: 91.97% | Val Acc: 84.34%\n",
            "[Epoch 59/250] Loss: 0.4554 | Train Acc: 92.12% | Val Acc: 84.26%\n",
            "[Epoch 60/250] Loss: 0.4498 | Train Acc: 92.36% | Val Acc: 84.56%\n",
            "[Epoch 61/250] Loss: 0.4498 | Train Acc: 92.31% | Val Acc: 84.44%\n",
            "[Epoch 62/250] Loss: 0.4429 | Train Acc: 92.67% | Val Acc: 86.00%\n",
            "[Epoch 63/250] Loss: 0.4407 | Train Acc: 92.65% | Val Acc: 85.42%\n",
            "[Epoch 64/250] Loss: 0.4375 | Train Acc: 92.82% | Val Acc: 85.48%\n",
            "[Epoch 65/250] Loss: 0.4323 | Train Acc: 93.08% | Val Acc: 86.36%\n",
            "[Epoch 66/250] Loss: 0.4296 | Train Acc: 93.08% | Val Acc: 85.36%\n",
            "[Epoch 67/250] Loss: 0.4251 | Train Acc: 93.30% | Val Acc: 85.12%\n",
            "[Epoch 68/250] Loss: 0.4216 | Train Acc: 93.46% | Val Acc: 86.64%\n",
            "[Epoch 69/250] Loss: 0.4199 | Train Acc: 93.64% | Val Acc: 85.60%\n",
            "[Epoch 70/250] Loss: 0.4170 | Train Acc: 93.74% | Val Acc: 86.48%\n",
            "[Epoch 71/250] Loss: 0.4100 | Train Acc: 94.07% | Val Acc: 84.84%\n",
            "[Epoch 72/250] Loss: 0.4119 | Train Acc: 93.89% | Val Acc: 84.72%\n",
            "[Epoch 73/250] Loss: 0.4063 | Train Acc: 94.16% | Val Acc: 86.00%\n",
            "[Epoch 74/250] Loss: 0.4041 | Train Acc: 94.27% | Val Acc: 85.10%\n",
            "[Epoch 75/250] Loss: 0.4011 | Train Acc: 94.33% | Val Acc: 84.78%\n",
            "[Epoch 76/250] Loss: 0.3972 | Train Acc: 94.49% | Val Acc: 86.18%\n",
            "[Epoch 77/250] Loss: 0.3962 | Train Acc: 94.52% | Val Acc: 86.52%\n",
            "[Epoch 78/250] Loss: 0.3915 | Train Acc: 94.77% | Val Acc: 86.82%\n",
            "[Epoch 79/250] Loss: 0.3884 | Train Acc: 94.83% | Val Acc: 86.98%\n",
            "[Epoch 80/250] Loss: 0.3842 | Train Acc: 95.02% | Val Acc: 86.02%\n",
            "[Epoch 81/250] Loss: 0.3861 | Train Acc: 94.94% | Val Acc: 85.94%\n",
            "[Epoch 82/250] Loss: 0.3808 | Train Acc: 95.18% | Val Acc: 87.16%\n",
            "[Epoch 83/250] Loss: 0.3804 | Train Acc: 95.08% | Val Acc: 87.20%\n",
            "[Epoch 84/250] Loss: 0.3778 | Train Acc: 95.30% | Val Acc: 87.50%\n",
            "[Epoch 85/250] Loss: 0.3749 | Train Acc: 95.36% | Val Acc: 85.54%\n",
            "[Epoch 86/250] Loss: 0.3722 | Train Acc: 95.44% | Val Acc: 85.96%\n",
            "[Epoch 87/250] Loss: 0.3720 | Train Acc: 95.45% | Val Acc: 86.36%\n",
            "[Epoch 88/250] Loss: 0.3668 | Train Acc: 95.62% | Val Acc: 86.38%\n",
            "[Epoch 89/250] Loss: 0.3647 | Train Acc: 95.88% | Val Acc: 86.56%\n",
            "[Epoch 90/250] Loss: 0.3619 | Train Acc: 95.85% | Val Acc: 85.94%\n",
            "[Epoch 91/250] Loss: 0.3595 | Train Acc: 95.94% | Val Acc: 87.62%\n",
            "[Epoch 92/250] Loss: 0.3580 | Train Acc: 96.20% | Val Acc: 86.76%\n",
            "[Epoch 93/250] Loss: 0.3571 | Train Acc: 96.07% | Val Acc: 86.30%\n",
            "[Epoch 94/250] Loss: 0.3544 | Train Acc: 96.16% | Val Acc: 86.26%\n",
            "[Epoch 95/250] Loss: 0.3539 | Train Acc: 96.10% | Val Acc: 86.66%\n",
            "[Epoch 96/250] Loss: 0.3513 | Train Acc: 96.23% | Val Acc: 87.48%\n",
            "[Epoch 97/250] Loss: 0.3479 | Train Acc: 96.48% | Val Acc: 86.84%\n",
            "[Epoch 98/250] Loss: 0.3495 | Train Acc: 96.40% | Val Acc: 86.00%\n",
            "[Epoch 99/250] Loss: 0.3451 | Train Acc: 96.64% | Val Acc: 86.76%\n",
            "[Epoch 100/250] Loss: 0.3425 | Train Acc: 96.69% | Val Acc: 86.06%\n",
            "[Epoch 101/250] Loss: 0.3389 | Train Acc: 96.84% | Val Acc: 85.74%\n",
            "[Epoch 102/250] Loss: 0.3386 | Train Acc: 96.88% | Val Acc: 87.46%\n",
            "[Epoch 103/250] Loss: 0.3375 | Train Acc: 96.78% | Val Acc: 87.34%\n",
            "[Epoch 104/250] Loss: 0.3343 | Train Acc: 96.94% | Val Acc: 87.78%\n",
            "[Epoch 105/250] Loss: 0.3343 | Train Acc: 96.97% | Val Acc: 88.54%\n",
            "[Epoch 106/250] Loss: 0.3323 | Train Acc: 97.06% | Val Acc: 87.26%\n",
            "[Epoch 107/250] Loss: 0.3309 | Train Acc: 97.12% | Val Acc: 87.28%\n",
            "[Epoch 108/250] Loss: 0.3295 | Train Acc: 97.23% | Val Acc: 87.42%\n",
            "[Epoch 109/250] Loss: 0.3285 | Train Acc: 97.20% | Val Acc: 87.94%\n",
            "[Epoch 110/250] Loss: 0.3242 | Train Acc: 97.44% | Val Acc: 87.20%\n",
            "[Epoch 111/250] Loss: 0.3241 | Train Acc: 97.39% | Val Acc: 85.96%\n",
            "[Epoch 112/250] Loss: 0.3239 | Train Acc: 97.37% | Val Acc: 86.98%\n",
            "[Epoch 113/250] Loss: 0.3199 | Train Acc: 97.47% | Val Acc: 87.10%\n",
            "[Epoch 114/250] Loss: 0.3214 | Train Acc: 97.51% | Val Acc: 87.50%\n",
            "[Epoch 115/250] Loss: 0.3188 | Train Acc: 97.65% | Val Acc: 87.68%\n",
            "[Epoch 116/250] Loss: 0.3200 | Train Acc: 97.59% | Val Acc: 88.04%\n",
            "[Epoch 117/250] Loss: 0.3150 | Train Acc: 97.71% | Val Acc: 87.26%\n",
            "[Epoch 118/250] Loss: 0.3142 | Train Acc: 97.74% | Val Acc: 86.34%\n",
            "[Epoch 119/250] Loss: 0.3157 | Train Acc: 97.70% | Val Acc: 87.58%\n",
            "[Epoch 120/250] Loss: 0.3128 | Train Acc: 97.88% | Val Acc: 88.14%\n",
            "[Epoch 121/250] Loss: 0.3114 | Train Acc: 97.89% | Val Acc: 88.02%\n",
            "[Epoch 122/250] Loss: 0.3098 | Train Acc: 97.97% | Val Acc: 87.60%\n",
            "[Epoch 123/250] Loss: 0.3081 | Train Acc: 97.96% | Val Acc: 87.32%\n",
            "[Epoch 124/250] Loss: 0.3094 | Train Acc: 97.93% | Val Acc: 87.78%\n",
            "[Epoch 125/250] Loss: 0.3090 | Train Acc: 97.93% | Val Acc: 87.40%\n",
            "[Epoch 126/250] Loss: 0.3062 | Train Acc: 98.05% | Val Acc: 87.78%\n",
            "[Epoch 127/250] Loss: 0.3038 | Train Acc: 98.10% | Val Acc: 87.60%\n",
            "[Epoch 128/250] Loss: 0.3022 | Train Acc: 98.27% | Val Acc: 88.40%\n",
            "[Epoch 129/250] Loss: 0.3016 | Train Acc: 98.23% | Val Acc: 87.90%\n",
            "[Epoch 130/250] Loss: 0.3006 | Train Acc: 98.22% | Val Acc: 87.66%\n",
            "[Epoch 131/250] Loss: 0.2989 | Train Acc: 98.34% | Val Acc: 87.94%\n",
            "[Epoch 132/250] Loss: 0.2994 | Train Acc: 98.33% | Val Acc: 87.02%\n",
            "[Epoch 133/250] Loss: 0.2983 | Train Acc: 98.33% | Val Acc: 87.68%\n",
            "[Epoch 134/250] Loss: 0.2967 | Train Acc: 98.38% | Val Acc: 88.70%\n",
            "[Epoch 135/250] Loss: 0.2957 | Train Acc: 98.47% | Val Acc: 88.30%\n",
            "[Epoch 136/250] Loss: 0.2948 | Train Acc: 98.47% | Val Acc: 88.52%\n",
            "[Epoch 137/250] Loss: 0.2941 | Train Acc: 98.48% | Val Acc: 88.26%\n",
            "[Epoch 138/250] Loss: 0.2920 | Train Acc: 98.56% | Val Acc: 87.70%\n",
            "[Epoch 139/250] Loss: 0.2935 | Train Acc: 98.54% | Val Acc: 88.36%\n",
            "[Epoch 140/250] Loss: 0.2912 | Train Acc: 98.69% | Val Acc: 88.26%\n",
            "[Epoch 141/250] Loss: 0.2912 | Train Acc: 98.64% | Val Acc: 88.70%\n",
            "[Epoch 142/250] Loss: 0.2907 | Train Acc: 98.62% | Val Acc: 87.72%\n",
            "[Epoch 143/250] Loss: 0.2902 | Train Acc: 98.62% | Val Acc: 88.74%\n",
            "[Epoch 144/250] Loss: 0.2878 | Train Acc: 98.73% | Val Acc: 88.34%\n",
            "[Epoch 145/250] Loss: 0.2859 | Train Acc: 98.77% | Val Acc: 88.24%\n",
            "[Epoch 146/250] Loss: 0.2863 | Train Acc: 98.78% | Val Acc: 87.56%\n",
            "[Epoch 147/250] Loss: 0.2867 | Train Acc: 98.80% | Val Acc: 88.74%\n",
            "[Epoch 148/250] Loss: 0.2870 | Train Acc: 98.75% | Val Acc: 87.50%\n",
            "[Epoch 149/250] Loss: 0.2856 | Train Acc: 98.81% | Val Acc: 88.14%\n",
            "[Epoch 150/250] Loss: 0.2835 | Train Acc: 98.85% | Val Acc: 88.06%\n",
            "[Epoch 151/250] Loss: 0.2831 | Train Acc: 98.88% | Val Acc: 88.16%\n",
            "[Epoch 152/250] Loss: 0.2816 | Train Acc: 98.91% | Val Acc: 89.16%\n",
            "[Epoch 153/250] Loss: 0.2816 | Train Acc: 98.96% | Val Acc: 88.66%\n",
            "[Epoch 154/250] Loss: 0.2809 | Train Acc: 98.97% | Val Acc: 87.92%\n",
            "[Epoch 155/250] Loss: 0.2799 | Train Acc: 99.04% | Val Acc: 88.26%\n",
            "[Epoch 156/250] Loss: 0.2786 | Train Acc: 99.07% | Val Acc: 88.68%\n",
            "[Epoch 157/250] Loss: 0.2799 | Train Acc: 98.93% | Val Acc: 89.24%\n",
            "[Epoch 158/250] Loss: 0.2769 | Train Acc: 99.14% | Val Acc: 89.10%\n",
            "[Epoch 159/250] Loss: 0.2761 | Train Acc: 99.11% | Val Acc: 88.42%\n",
            "[Epoch 160/250] Loss: 0.2772 | Train Acc: 99.05% | Val Acc: 88.82%\n",
            "[Epoch 161/250] Loss: 0.2759 | Train Acc: 99.12% | Val Acc: 88.70%\n",
            "[Epoch 162/250] Loss: 0.2749 | Train Acc: 99.15% | Val Acc: 89.42%\n",
            "[Epoch 163/250] Loss: 0.2747 | Train Acc: 99.14% | Val Acc: 88.98%\n",
            "[Epoch 164/250] Loss: 0.2741 | Train Acc: 99.20% | Val Acc: 88.94%\n",
            "[Epoch 165/250] Loss: 0.2725 | Train Acc: 99.26% | Val Acc: 89.06%\n",
            "[Epoch 166/250] Loss: 0.2725 | Train Acc: 99.16% | Val Acc: 89.48%\n",
            "[Epoch 167/250] Loss: 0.2715 | Train Acc: 99.32% | Val Acc: 88.70%\n",
            "[Epoch 168/250] Loss: 0.2715 | Train Acc: 99.30% | Val Acc: 88.08%\n",
            "[Epoch 169/250] Loss: 0.2697 | Train Acc: 99.34% | Val Acc: 88.64%\n",
            "[Epoch 170/250] Loss: 0.2710 | Train Acc: 99.25% | Val Acc: 89.44%\n",
            "[Epoch 171/250] Loss: 0.2704 | Train Acc: 99.30% | Val Acc: 88.94%\n",
            "[Epoch 172/250] Loss: 0.2689 | Train Acc: 99.32% | Val Acc: 88.86%\n",
            "[Epoch 173/250] Loss: 0.2698 | Train Acc: 99.31% | Val Acc: 89.10%\n",
            "[Epoch 174/250] Loss: 0.2687 | Train Acc: 99.36% | Val Acc: 90.00%\n",
            "[Epoch 175/250] Loss: 0.2684 | Train Acc: 99.42% | Val Acc: 89.36%\n",
            "[Epoch 176/250] Loss: 0.2665 | Train Acc: 99.44% | Val Acc: 89.08%\n",
            "[Epoch 177/250] Loss: 0.2672 | Train Acc: 99.41% | Val Acc: 89.02%\n",
            "[Epoch 178/250] Loss: 0.2657 | Train Acc: 99.42% | Val Acc: 88.80%\n",
            "[Epoch 179/250] Loss: 0.2667 | Train Acc: 99.43% | Val Acc: 89.02%\n",
            "[Epoch 180/250] Loss: 0.2666 | Train Acc: 99.41% | Val Acc: 89.02%\n",
            "[Epoch 181/250] Loss: 0.2659 | Train Acc: 99.42% | Val Acc: 89.40%\n",
            "[Epoch 182/250] Loss: 0.2654 | Train Acc: 99.45% | Val Acc: 89.40%\n",
            "[Epoch 183/250] Loss: 0.2642 | Train Acc: 99.52% | Val Acc: 89.32%\n",
            "[Epoch 184/250] Loss: 0.2640 | Train Acc: 99.50% | Val Acc: 89.12%\n",
            "[Epoch 185/250] Loss: 0.2647 | Train Acc: 99.42% | Val Acc: 89.42%\n",
            "[Epoch 186/250] Loss: 0.2624 | Train Acc: 99.56% | Val Acc: 89.24%\n",
            "[Epoch 187/250] Loss: 0.2630 | Train Acc: 99.53% | Val Acc: 89.22%\n",
            "[Epoch 188/250] Loss: 0.2634 | Train Acc: 99.46% | Val Acc: 89.68%\n",
            "[Epoch 189/250] Loss: 0.2624 | Train Acc: 99.54% | Val Acc: 89.22%\n",
            "[Epoch 190/250] Loss: 0.2621 | Train Acc: 99.51% | Val Acc: 88.98%\n",
            "[Epoch 191/250] Loss: 0.2615 | Train Acc: 99.58% | Val Acc: 88.98%\n",
            "[Epoch 192/250] Loss: 0.2618 | Train Acc: 99.58% | Val Acc: 89.22%\n",
            "[Epoch 193/250] Loss: 0.2604 | Train Acc: 99.58% | Val Acc: 89.20%\n",
            "[Epoch 194/250] Loss: 0.2604 | Train Acc: 99.61% | Val Acc: 89.00%\n",
            "[Epoch 195/250] Loss: 0.2612 | Train Acc: 99.54% | Val Acc: 89.72%\n",
            "[Epoch 196/250] Loss: 0.2609 | Train Acc: 99.59% | Val Acc: 89.50%\n",
            "[Epoch 197/250] Loss: 0.2605 | Train Acc: 99.61% | Val Acc: 89.96%\n",
            "[Epoch 198/250] Loss: 0.2605 | Train Acc: 99.58% | Val Acc: 89.48%\n",
            "[Epoch 199/250] Loss: 0.2596 | Train Acc: 99.63% | Val Acc: 89.06%\n",
            "[Epoch 200/250] Loss: 0.2593 | Train Acc: 99.65% | Val Acc: 89.86%\n",
            "[Epoch 201/250] Loss: 0.2583 | Train Acc: 99.67% | Val Acc: 88.96%\n",
            "[Epoch 202/250] Loss: 0.2581 | Train Acc: 99.63% | Val Acc: 89.88%\n",
            "[Epoch 203/250] Loss: 0.2585 | Train Acc: 99.64% | Val Acc: 89.46%\n",
            "[Epoch 204/250] Loss: 0.2585 | Train Acc: 99.63% | Val Acc: 89.60%\n",
            "[Epoch 205/250] Loss: 0.2585 | Train Acc: 99.66% | Val Acc: 89.32%\n",
            "[Epoch 206/250] Loss: 0.2578 | Train Acc: 99.69% | Val Acc: 90.12%\n",
            "[Epoch 207/250] Loss: 0.2574 | Train Acc: 99.68% | Val Acc: 89.56%\n",
            "[Epoch 208/250] Loss: 0.2576 | Train Acc: 99.68% | Val Acc: 89.62%\n",
            "[Epoch 209/250] Loss: 0.2570 | Train Acc: 99.68% | Val Acc: 89.62%\n",
            "[Epoch 210/250] Loss: 0.2572 | Train Acc: 99.69% | Val Acc: 89.44%\n",
            "[Epoch 211/250] Loss: 0.2571 | Train Acc: 99.73% | Val Acc: 89.26%\n",
            "[Epoch 212/250] Loss: 0.2565 | Train Acc: 99.72% | Val Acc: 89.36%\n",
            "[Epoch 213/250] Loss: 0.2565 | Train Acc: 99.70% | Val Acc: 89.48%\n",
            "[Epoch 214/250] Loss: 0.2560 | Train Acc: 99.71% | Val Acc: 89.62%\n",
            "[Epoch 215/250] Loss: 0.2555 | Train Acc: 99.73% | Val Acc: 89.56%\n",
            "[Epoch 216/250] Loss: 0.2559 | Train Acc: 99.75% | Val Acc: 89.60%\n",
            "[Epoch 217/250] Loss: 0.2564 | Train Acc: 99.66% | Val Acc: 89.48%\n",
            "[Epoch 218/250] Loss: 0.2555 | Train Acc: 99.73% | Val Acc: 89.94%\n",
            "[Epoch 219/250] Loss: 0.2555 | Train Acc: 99.72% | Val Acc: 89.28%\n",
            "[Epoch 220/250] Loss: 0.2557 | Train Acc: 99.69% | Val Acc: 89.78%\n",
            "[Epoch 221/250] Loss: 0.2553 | Train Acc: 99.71% | Val Acc: 89.56%\n",
            "[Epoch 222/250] Loss: 0.2564 | Train Acc: 99.72% | Val Acc: 89.40%\n",
            "[Epoch 223/250] Loss: 0.2561 | Train Acc: 99.69% | Val Acc: 89.88%\n",
            "[Epoch 224/250] Loss: 0.2546 | Train Acc: 99.74% | Val Acc: 89.90%\n",
            "[Epoch 225/250] Loss: 0.2551 | Train Acc: 99.74% | Val Acc: 89.20%\n",
            "[Epoch 226/250] Loss: 0.2548 | Train Acc: 99.74% | Val Acc: 89.76%\n",
            "[Epoch 227/250] Loss: 0.2549 | Train Acc: 99.73% | Val Acc: 89.66%\n",
            "[Epoch 228/250] Loss: 0.2539 | Train Acc: 99.79% | Val Acc: 89.22%\n",
            "[Epoch 229/250] Loss: 0.2545 | Train Acc: 99.76% | Val Acc: 90.00%\n",
            "[Epoch 230/250] Loss: 0.2542 | Train Acc: 99.76% | Val Acc: 90.32%\n",
            "[Epoch 231/250] Loss: 0.2536 | Train Acc: 99.81% | Val Acc: 89.64%\n",
            "[Epoch 232/250] Loss: 0.2539 | Train Acc: 99.82% | Val Acc: 90.36%\n",
            "[Epoch 233/250] Loss: 0.2549 | Train Acc: 99.75% | Val Acc: 90.04%\n",
            "[Epoch 234/250] Loss: 0.2541 | Train Acc: 99.78% | Val Acc: 89.60%\n",
            "[Epoch 235/250] Loss: 0.2534 | Train Acc: 99.80% | Val Acc: 89.66%\n",
            "[Epoch 236/250] Loss: 0.2543 | Train Acc: 99.76% | Val Acc: 89.80%\n",
            "[Epoch 237/250] Loss: 0.2539 | Train Acc: 99.80% | Val Acc: 89.84%\n",
            "[Epoch 238/250] Loss: 0.2537 | Train Acc: 99.76% | Val Acc: 89.74%\n",
            "[Epoch 239/250] Loss: 0.2542 | Train Acc: 99.77% | Val Acc: 89.38%\n",
            "[Epoch 240/250] Loss: 0.2542 | Train Acc: 99.77% | Val Acc: 90.08%\n",
            "[Epoch 241/250] Loss: 0.2538 | Train Acc: 99.79% | Val Acc: 89.38%\n",
            "[Epoch 242/250] Loss: 0.2535 | Train Acc: 99.78% | Val Acc: 90.00%\n",
            "[Epoch 243/250] Loss: 0.2531 | Train Acc: 99.79% | Val Acc: 89.22%\n",
            "[Epoch 244/250] Loss: 0.2531 | Train Acc: 99.81% | Val Acc: 89.84%\n",
            "[Epoch 245/250] Loss: 0.2536 | Train Acc: 99.79% | Val Acc: 89.68%\n",
            "[Epoch 246/250] Loss: 0.2535 | Train Acc: 99.80% | Val Acc: 90.00%\n",
            "[Epoch 247/250] Loss: 0.2543 | Train Acc: 99.77% | Val Acc: 89.32%\n",
            "[Epoch 248/250] Loss: 0.2529 | Train Acc: 99.79% | Val Acc: 89.56%\n",
            "[Epoch 249/250] Loss: 0.2538 | Train Acc: 99.76% | Val Acc: 90.12%\n",
            "[Epoch 250/250] Loss: 0.2542 | Train Acc: 99.75% | Val Acc: 89.50%\n",
            "\n",
            "Distilled Student Test Accuracy: 90.57%\n",
            "\n",
            "=== Generating Submission ===\n",
            "Saved predictions to submission.csv\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) Data\n",
        "    trainloader, valloader, testloader = get_dataloaders(batch_size=512, val_ratio=0.1)\n",
        "\n",
        "    # 2) Teacher\n",
        "    teacher = ResNetTeacher()\n",
        "    print(\"\\n=== Training Teacher ===\")\n",
        "    teacher = train_teacher(\n",
        "        teacher_model=teacher,\n",
        "        trainloader=trainloader,\n",
        "        valloader=valloader,\n",
        "        num_epochs=100,        # Adjust as needed\n",
        "        lr=0.1,                # Common for CIFAR-10 with ResNets\n",
        "        label_smoothing=0.0    # or 0.1 if you like label smoothing\n",
        "    )\n",
        "\n",
        "    # If you already have a teacher checkpoint, load it:\n",
        "    # teacher.load_state_dict(torch.load('path_to_teacher.pth'))\n",
        "    # teacher.eval()\n",
        "\n",
        "    # 3) Student\n",
        "    student = ResNetStudent()\n",
        "\n",
        "    # 4) Distillation\n",
        "    print(\"\\n=== Distillation Training ===\")\n",
        "    student = train_distilled(\n",
        "        teacher,\n",
        "        student,\n",
        "        trainloader,\n",
        "        valloader,\n",
        "        num_epochs=250,\n",
        "        alpha=0.4,   # Tweak as needed\n",
        "        T=1.0,       # Tweak as needed\n",
        "        lr=0.01,\n",
        "        label_smoothing=0.1\n",
        "    )\n",
        "\n",
        "    # 5) Evaluate on CIFAR-10 test\n",
        "    test_acc = validate(student, testloader, device)\n",
        "    print(f\"\\nDistilled Student Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # (Optional) Save student checkpoint\n",
        "    torch.save(student.state_dict(), \"distilled_student.pth\")\n",
        "\n",
        "    # 6) Generate submission for unlabeled data\n",
        "    # Example: 'drive/MyDrive/cifar_test_nolabel.pkl'\n",
        "    try:\n",
        "        ids, imgs = load_eval('drive/MyDrive/cifar_test_nolabel.pkl')\n",
        "        transform_unlabeled = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2470, 0.2435, 0.2616)),\n",
        "        ])\n",
        "        eval_dataset = EvalDataset(ids, imgs, transform_unlabeled)\n",
        "        eval_loader  = DataLoader(eval_dataset, batch_size=2048, shuffle=False)\n",
        "\n",
        "        print(\"\\n=== Generating Submission ===\")\n",
        "        generate_submission(student, eval_loader, device, out_csv='submission.csv')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No unlabeled .pkl file found. Skipping submission step.\")\n",
        "\n",
        "def submiss():\n",
        "    student = ResNetStudent()\n",
        "    student.load_state_dict(torch.load('distilled_student.pth'))\n",
        "    student = student.to(device) # Move the student model to the device (GPU)\n",
        "    student.eval()\n",
        "    try:\n",
        "        ids, imgs = load_eval('drive/MyDrive/cifar_test_nolabel.pkl')\n",
        "        transform_unlabeled = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2470, 0.2435, 0.2616)),\n",
        "        ])\n",
        "        eval_dataset = EvalDataset(ids, imgs, transform_unlabeled)\n",
        "        eval_loader  = DataLoader(eval_dataset, batch_size=2048, shuffle=False)\n",
        "\n",
        "        print(\"\\n=== Generating Submission ===\")\n",
        "        generate_submission(student, eval_loader, device, out_csv='submission.csv')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No unlabeled .pkl file found. Skipping submission step.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # submiss()\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}